---
permalink: /
title: "About Me"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I'm a undergraduate student in [EECS at UC Berkeley](https://www.eecs.berkeley.edu), where I'm fortunate to be advised by [Emmanouil-Vasileios Vlatakis-Gkaragkounis](http://www.cs.columbia.edu/~emvlatakis/index.html).  

My research interests broadly lie in <b> extending stochastic optimization into non-convex regines</b>. More specifically, I'm interested in <i>developing theory to understand, improve, and simplify empirical optimization methodology</i>. 

I'm particularly interested in problem instances where data is high-dimensional yet has rich structure.




<!-- Recent Updates
=====
- (November 2023) [New (comprehensive) pre-print](https://arxiv.org/abs/2311.13110) reviewing our ``White-Box Transformers'' line of work: deriving efficient, interpretable, and performant transformer-like architectures from first-principles information theory and signal processing.
- (November 2023) Our papers [Emergence of Segmentation with Minimalistic White-Box Transformers](https://arxiv.org/abs/2308.16271), [Closed-Loop Transcription via Convolutional Sparse Coding
](https://arxiv.org/abs/2302.09347), and [Masked Completion via Structured Diffusion with White-Box Transformers](https://openreview.net/forum?id=PvyOYleymy) were accepted to [CPAL 2024](https://cpal.cc/).
- (October 2023) Our paper [Emergence of Segmentation with Minimalistic White-Box Transformers](https://arxiv.org/abs/2308.16271) was accepted to [NeurIPS 2023 XAIA Workshop](https://neurips.cc/virtual/2023/workshop/66529).
- (September 2023) Our paper [White-Box Transformers via Sparse Rate Reduction](https://arxiv.org/abs/2306.01129), proposing an interpretable and parameter-efficient transformer-like architecture derived from first-principles, was accepted to [NeurIPS 2023](https://neurips.cc/).
- (August 2023) Started my Ph.D. program in EECS at UC Berkeley! -->
